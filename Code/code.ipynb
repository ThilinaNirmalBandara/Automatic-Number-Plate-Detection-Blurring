{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e912a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "85075b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fd4bf5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberPlateDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, transform=None, max_images=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Filter images that have a corresponding XML with <object>\n",
    "        self.image_files = []\n",
    "        for f in os.listdir(images_dir):\n",
    "            if not f.endswith('.jpg'):\n",
    "                continue\n",
    "            xml_path = os.path.join(annotations_dir, f.replace('.jpg', '.xml'))\n",
    "            if not os.path.exists(xml_path):\n",
    "                continue\n",
    "            try:\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "                if root.find('object') is not None:  # include only if object exists\n",
    "                    self.image_files.append(f)\n",
    "            except ET.ParseError:\n",
    "                # skip XML files that are malformed\n",
    "                continue\n",
    "        \n",
    "        # Limit the number of images\n",
    "        if max_images:\n",
    "            self.image_files = self.image_files[:max_images]\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        height, width, _ = image.shape\n",
    "        \n",
    "        # Parse XML\n",
    "        xml_path = os.path.join(self.annotations_dir, img_name.replace('.jpg','.xml'))\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        obj = root.find('object')\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) / width    # Normalize\n",
    "        ymin = int(bbox.find('ymin').text) / height\n",
    "        xmax = int(bbox.find('xmax').text) / width\n",
    "        ymax = int(bbox.find('ymax').text) / height\n",
    "        target = torch.tensor([xmin, ymin, xmax, ymax], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b79d4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e58236fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NumberPlateDataset(\n",
    "    images_dir='C:/Users/thili/OneDrive/Desktop/NumberPlateDetection&Blur/train', \n",
    "    annotations_dir='C:/Users/thili/OneDrive/Desktop/NumberPlateDetection&Blur/train', \n",
    "    transform=transform,\n",
    "    max_images=1000\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a41473dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,16,3,padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(16,32,3,padding=1)\n",
    "        self.fc1 = nn.Linear(32*32*32, 128)\n",
    "        self.fc2 = nn.Linear(128, 4)  # output: [xmin, ymin, xmax, ymax]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32*32*32)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d0eb660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SmoothL1Loss()  # Predict bbox coordinates\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "23ec7143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0147\n",
      "Epoch 2, Loss: 0.0132\n",
      "Epoch 3, Loss: 0.0155\n",
      "Epoch 4, Loss: 0.0124\n",
      "Epoch 5, Loss: 0.0101\n",
      "Epoch 6, Loss: 0.0050\n",
      "Epoch 7, Loss: 0.0092\n",
      "Epoch 8, Loss: 0.0050\n",
      "Epoch 9, Loss: 0.0071\n",
      "Epoch 10, Loss: 0.0030\n",
      "Epoch 11, Loss: 0.0032\n",
      "Epoch 12, Loss: 0.0061\n",
      "Epoch 13, Loss: 0.0048\n",
      "Epoch 14, Loss: 0.0009\n",
      "Epoch 15, Loss: 0.0053\n",
      "Epoch 16, Loss: 0.0019\n",
      "Epoch 17, Loss: 0.0020\n",
      "Epoch 18, Loss: 0.0014\n",
      "Epoch 19, Loss: 0.0011\n",
      "Epoch 20, Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):  # example epochs\n",
    "    for images, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "52ca94dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=32768, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d79bb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Path to test images folder\n",
    "test_folder = \"C:/Users/thili/OneDrive/Desktop/NumberPlateDetection&Blur/test\"\n",
    "test_images = [f for f in os.listdir(test_folder) if f.endswith('.jpg')][:20]  # first 20 images\n",
    "\n",
    "max_size = 1000\n",
    "min_size = 500\n",
    "\n",
    "for img_file in test_images:\n",
    "    img_path = os.path.join(test_folder, img_file)\n",
    "    image = cv2.imread(img_path)\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Resize if smaller than 500x500, but not larger than 1000x1000\n",
    "    scale = 1.0\n",
    "    if width < min_size or height < min_size:\n",
    "        scale = max(min_size / width, min_size / height)\n",
    "    elif width > max_size or height > max_size:\n",
    "        scale = min(max_size / width, max_size / height)\n",
    "\n",
    "    if scale != 1.0:\n",
    "        new_width = int(width * scale)\n",
    "        new_height = int(height * scale)\n",
    "        image = cv2.resize(image, (new_width, new_height))\n",
    "        height, width = new_height, new_width\n",
    "\n",
    "    # Convert to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert to tensor and normalize\n",
    "    image_tensor = transform(image_rgb)  # Apply your training transforms\n",
    "    image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_bbox = model(image_tensor)\n",
    "\n",
    "    # Convert predicted bbox to pixel coordinates\n",
    "    pred_bbox = pred_bbox.squeeze(0).numpy()\n",
    "    xmin = int(pred_bbox[0] * width)\n",
    "    ymin = int(pred_bbox[1] * height)\n",
    "    xmax = int(pred_bbox[2] * width)\n",
    "    ymax = int(pred_bbox[3] * height)\n",
    "\n",
    "    # Draw rectangle\n",
    "    cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Prediction\", image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
